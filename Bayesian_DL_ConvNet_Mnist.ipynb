{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bayesian_DL_ConvNet_Mnist",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/philarnold4242/test_Python/blob/master/Bayesian_DL_ConvNet_Mnist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "RBWclJu3nJm1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Chapter 4 - Convolutional Neural Nets<br>\n",
        "https://www.tensorflow.org/tutorials/deep_cnn"
      ]
    },
    {
      "metadata": {
        "id": "7Lx8J8xrSf8x",
        "colab_type": "code",
        "outputId": "15fd8f4e-1a37-4986-ace8-2b79b81a7be3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZAPEMIOayy-s",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Uncertainties<br>\n",
        "Helper functions"
      ]
    },
    {
      "metadata": {
        "id": "K8i4CAtpy9dj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Pseudocount - needs thinking.\n",
        "pc = 1e-10\n",
        "\n",
        "def softmax(x):\n",
        "    \"\"\" Multinomial logisitc fuction. \"\"\"\n",
        "    mmax = np.amax(x, axis=1).reshape(x.shape[0], 1)\n",
        "    ex = np.exp(x - mmax)\n",
        "    ex_sum = ex.sum(axis=1).reshape(ex.shape[0],1)\n",
        "    return ex / ex_sum\n",
        "    \n",
        "\n",
        "def var_ratio(x):\n",
        "    \"\"\" How spread is the distribution around the mode?\"\"\" \n",
        "    ratios = np.zeros(shape=(x.shape[1]))\n",
        "    \n",
        "    # Loop over test examples\n",
        "    for n in range(x.shape[1]):\n",
        "        \n",
        "        # Get predicted class\n",
        "        samples_n = np.argmax(x[:, n], axis=1)\n",
        "        \n",
        "        # Get most abundant class\n",
        "        count = np.argmax(np.bincount(samples_n))\n",
        "        \n",
        "        # Return frequency of class\n",
        "        ratios[n] = np.sum(samples_n==count) / x.shape[0]\n",
        "\n",
        "    return ratios\n",
        "\n",
        "\n",
        "def pred_ent(x):\n",
        "    \"\"\" Average amount of information contained in posterior predictive distribution. \"\"\"\n",
        "    \n",
        "    N = x.shape[1] # Number of test cases\n",
        "    T = x.shape[0] # Number of samples per test case\n",
        "    C = 10 # number of classes\n",
        "    \n",
        "    entropies = np.zeros(shape=(x.shape[1]))\n",
        "    \n",
        "    # Loop over test examples\n",
        "    for n in range(N):\n",
        "        \n",
        "        # Approx posterior predictive distribution\n",
        "        ppd = np.mean(x[:,n,:] + pc, axis=0)\n",
        "        \n",
        "        # Calculate predictive entropy\n",
        "        entropies[n] = - np.sum(np.multiply(ppd, np.log(ppd)))\n",
        "    \n",
        "    return entropies\n",
        "\n",
        "\n",
        "def mut_inf(x):\n",
        "    \"\"\" MI between prediction y_pred and posterior distribution for the model parameters. \"\"\"\n",
        "    \n",
        "    N = x.shape[1] # Number of test cases\n",
        "    T = x.shape[0] # Number of samples per test case\n",
        "    C = 10 # number of classes\n",
        "        \n",
        "    mis = np.zeros(shape=(x.shape[1]))\n",
        "    \n",
        "    # Loop over test examples\n",
        "    for n in range(N):\n",
        "        \n",
        "        # Approx posterior predictive distribution\n",
        "        ppd = np.mean(x[:,n,:] + pc, axis=0)\n",
        "        \n",
        "        # Calculate predictive entropy\n",
        "        pe = - np.sum(np.multiply(ppd, np.log(ppd)))\n",
        "        \n",
        "        # Calculate conditional entropy        \n",
        "        p = x[:,n,:] + pc\n",
        "        logp = np.log(p)\n",
        "        ce = np.sum(np.multiply(p,logp))\n",
        "        \n",
        "        mis[n] = 1/T * pe + ce\n",
        "    \n",
        "    return mis"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "14cQDiYv1SpU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **Let's get started**"
      ]
    },
    {
      "metadata": {
        "id": "VO9aw8uUokBW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Set up modules"
      ]
    },
    {
      "metadata": {
        "id": "jkgZW6xuuTuo",
        "colab_type": "code",
        "outputId": "f724340c-5196-4e06-9a29-1850ef2d8931",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "np.set_printoptions(precision=5)\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import pickle\n",
        "import urllib\n",
        "import tarfile\n",
        "\n",
        "!pip install git+git://github.com/rasbt/mlxtend.git\n",
        "from mlxtend.data import loadlocal_mnist\n",
        "\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+git://github.com/rasbt/mlxtend.git\n",
            "  Cloning git://github.com/rasbt/mlxtend.git to /tmp/pip-req-build-s3i5bhmk\n",
            "Requirement already satisfied (use --upgrade to upgrade): mlxtend==0.15.0.dev0 from git+git://github.com/rasbt/mlxtend.git in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: scipy>=0.17 in /usr/local/lib/python3.6/dist-packages (from mlxtend==0.15.0.dev0) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from mlxtend==0.15.0.dev0) (1.14.6)\n",
            "Requirement already satisfied: pandas>=0.17.1 in /usr/local/lib/python3.6/dist-packages (from mlxtend==0.15.0.dev0) (0.22.0)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.6/dist-packages (from mlxtend==0.15.0.dev0) (0.19.2)\n",
            "Requirement already satisfied: matplotlib>=1.5.1 in /usr/local/lib/python3.6/dist-packages (from mlxtend==0.15.0.dev0) (2.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from mlxtend==0.15.0.dev0) (40.6.2)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas>=0.17.1->mlxtend==0.15.0.dev0) (2018.7)\n",
            "Requirement already satisfied: python-dateutil>=2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.17.1->mlxtend==0.15.0.dev0) (2.5.3)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.5.1->mlxtend==0.15.0.dev0) (1.11.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.5.1->mlxtend==0.15.0.dev0) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.5.1->mlxtend==0.15.0.dev0) (2.3.0)\n",
            "Building wheels for collected packages: mlxtend\n",
            "  Running setup.py bdist_wheel for mlxtend ... \u001b[?25l-\b \b\\\b \b|\b \bdone\n",
            "\u001b[?25h  Stored in directory: /tmp/pip-ephem-wheel-cache-rtn9xxr0/wheels/85/e4/4c/ee71547ac9ea223b07fe8f55b0e5f71536a6a34ae3480205f3\n",
            "Successfully built mlxtend\n",
            "1.12.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3ZWo6tDooLq5",
        "colab_type": "code",
        "outputId": "135e809a-c95b-44cd-f77a-840e4fe2ff9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "cell_type": "code",
      "source": [
        "## Check GPU\n",
        "with tf.device('/gpu:0'):\n",
        "    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n",
        "    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n",
        "    c = tf.matmul(a, b)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    print (sess.run(c))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[22. 28.]\n",
            " [49. 64.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YhH96ZrWoZAu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### ConvNet - The hard way"
      ]
    },
    {
      "metadata": {
        "id": "GDzeh2PXykbP",
        "colab_type": "code",
        "outputId": "c44db710-f348-4379-a9d3-d672b9be68fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        }
      },
      "cell_type": "code",
      "source": [
        "## Load MNIST\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-5-93d8da72a918>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3kNaG8xHt78M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Helper functions\n",
        "def weight_variable(shape):\n",
        "    \"\"\" Specifies the weight for either fully-connected or conv layers.\"\"\"\n",
        "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
        "    return tf.Variable(initial)\n",
        "\n",
        "\n",
        "def bias_variable(shape):\n",
        "    \"\"\" Bias elements. \"\"\"\n",
        "    initial = tf.constant(0.1, shape=shape)\n",
        "    return tf.Variable(initial)\n",
        "\n",
        "\n",
        "def conv2d(x, W):\n",
        "    \"\"\" This specifies a full convolution with an output the same size\n",
        "        as the input x. \n",
        "        [batch, height, width, channels] \"\"\"\n",
        "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
        "\n",
        "\n",
        "def max_pool_2x2(x):\n",
        "    \"\"\" ksize: A 1-D int Tensor of 4 elements. The size of the window for each \n",
        "               dimension of the input tensor.\n",
        "        strides: A 1-D int Tensor of 4 elements. The stride of the sliding window for \n",
        "                 each dimension of the input tensor. \"\"\"\n",
        "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "i5smh_tbuKh_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Architecture parameters\n",
        "pic_res = 28\n",
        "n_channel = 1\n",
        "\n",
        "# Placeholders to feed stuff to network\n",
        "x = tf.placeholder(tf.float32, shape=[None, pic_res*pic_res])\n",
        "y_true = tf.placeholder(tf.float32, shape=[None, 10])\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "\n",
        "\n",
        "N = 200 # Number of samples for forward pass\n",
        "Ntrain_steps = 20000\n",
        "bayesian_keep_prob = 0.8"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fO5bdhz9uQsq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_convnet(do_bayes = False):\n",
        "  ## The net\n",
        "  # Remember, the input image is a 1D tensor of length 784.\n",
        "  mnist.train.images[1].shape\n",
        "\n",
        "  # To apply the layer, we first reshape x to a 4d tensor, with the second and \n",
        "  # third dimensions corresponding to image width and height, and the final dimension \n",
        "  # corresponding to the number of color channels.\n",
        "  x_image = tf.reshape(x, [-1, pic_res, pic_res, n_channel])\n",
        "\n",
        "  # First conv layer\n",
        "  # The convolution will compute 32 features for each 3x3 patch. Its weight tensor will have a \n",
        "  # shape of [3, 3, 1, 32]. The first two dimensions are the patch size, the next is the number \n",
        "  # of input channels, and the last is the number of output channels. \n",
        "  W_conv1 = weight_variable([3, 3, n_channel, 32])\n",
        "  b_conv1 = bias_variable([32])\n",
        "  conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
        "  if do_bayes:\n",
        "    conv1 = tf.nn.dropout(conv1, keep_prob=keep_prob)\n",
        "  h_pool1 = max_pool_2x2(conv1)\n",
        "\n",
        "  # Second conv layer\n",
        "  W_conv2 = weight_variable([3, 3, 32, 64])\n",
        "  b_conv2 = bias_variable([64])\n",
        "  conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
        "  if do_bayes:\n",
        "    conv2 = tf.nn.dropout(conv2, keep_prob=keep_prob)\n",
        "  h_pool2 = max_pool_2x2(conv2)\n",
        "\n",
        "  # Dense layer \n",
        "  pic_res4_sq = int(pic_res/4)*int(pic_res/4)\n",
        "  W_fc1 = weight_variable([pic_res4_sq * 64, 1024])\n",
        "  b_fc1 = bias_variable([1024])\n",
        "\n",
        "  h_pool2_flat = tf.reshape(h_pool2, [-1, pic_res4_sq*64])\n",
        "  h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
        "  # Dropout\n",
        "  h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob=keep_prob)\n",
        "\n",
        "  # Output\n",
        "  W_fc2 = weight_variable([1024, 10])\n",
        "  b_fc2 = bias_variable([10])\n",
        "\n",
        "  h_fc2 = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
        "  h_fc2_drop = tf.nn.dropout(h_fc2, keep_prob=keep_prob)\n",
        "  y_conv = h_fc2_drop\n",
        "  \n",
        "  return(y_conv)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "STVMxtaMC4y8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Tranditional ConvNet"
      ]
    },
    {
      "metadata": {
        "id": "vOnCcPGfuWIo",
        "colab_type": "code",
        "outputId": "c7706b7e-1800-404b-e3e8-56aee9ae0714",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        }
      },
      "cell_type": "code",
      "source": [
        "# Objective function\n",
        "y_conv = create_convnet(do_bayes = False)\n",
        "y_pred = tf.nn.softmax_cross_entropy_with_logits(labels=y_true, logits=y_conv)\n",
        "cross_entropy = tf.reduce_mean(y_pred)\n",
        "\n",
        "# Optimizer\n",
        "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
        "\n",
        "# Evaluation\n",
        "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_true, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-9-1d222a8a8337>:2: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YxYiTuaruedH",
        "colab_type": "code",
        "outputId": "a62e4a2a-f453-4bff-daf6-e896092e7d5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        }
      },
      "cell_type": "code",
      "source": [
        "## Train - MNIST\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "for i in range(int(Ntrain_steps)+1):\n",
        "    batch = mnist.train.next_batch(64)\n",
        "\n",
        "    # Training\n",
        "    _, loss = sess.run([train_step, cross_entropy], \n",
        "                       feed_dict={x: batch[0], y_true: batch[1], \n",
        "                                  keep_prob: float(bayesian_keep_prob)})\n",
        "    \n",
        "    # Status report\n",
        "    if i % 1000 == 0:\n",
        "        train_acc = sess.run(accuracy, feed_dict={x: batch[0], \n",
        "                                                  y_true: batch[1], \n",
        "                                                  keep_prob: float(1.0)})\n",
        "        \n",
        "        test_acc = sess.run(accuracy, feed_dict={x: mnist.test.images,\n",
        "                                                  y_true: mnist.test.labels, \n",
        "                                                  keep_prob: 1.0})\n",
        "        info = [str(tx) for tx in [i, loss, train_acc, test_acc] ]\n",
        "        print('\\t'.join(info))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\t4.7984467\t0.109375\t0.1128\n",
            "1000\t0.3629266\t1.0\t0.9586\n",
            "2000\t0.27385744\t0.96875\t0.9723\n",
            "3000\t0.28072077\t1.0\t0.9817\n",
            "4000\t0.26521128\t0.984375\t0.9822\n",
            "5000\t0.21265593\t0.984375\t0.9869\n",
            "6000\t0.3065703\t0.984375\t0.9887\n",
            "7000\t0.17142633\t1.0\t0.9876\n",
            "8000\t0.23067905\t1.0\t0.9891\n",
            "9000\t0.15771323\t1.0\t0.99\n",
            "10000\t0.2611502\t1.0\t0.9905\n",
            "11000\t0.19413567\t1.0\t0.9893\n",
            "12000\t0.11987758\t1.0\t0.9908\n",
            "13000\t0.27303657\t1.0\t0.9914\n",
            "14000\t0.24158649\t1.0\t0.9901\n",
            "15000\t0.14956823\t1.0\t0.9904\n",
            "16000\t0.22842099\t0.984375\t0.9911\n",
            "17000\t0.20375058\t1.0\t0.9912\n",
            "18000\t0.14236963\t1.0\t0.9911\n",
            "19000\t0.19692509\t1.0\t0.9914\n",
            "20000\t0.13233367\t1.0\t0.992\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "19rtEjfLDNA6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Get predictions\n",
        "y_prob = tf.nn.softmax(y_conv)\n",
        "pred_trad = sess.run(y_prob, feed_dict={x: mnist.test.images,\n",
        "                                          y_true: mnist.test.labels, \n",
        "                                          keep_prob: 1.0})\n",
        "\n",
        "\n",
        "labels_trad = np.argmax(pred_trad, axis=1)\n",
        "probs_trad = np.max(pred_trad, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TtkidrwlESA8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Bayesian ConvNet"
      ]
    },
    {
      "metadata": {
        "id": "RJSvS1LyEZWx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Objective function\n",
        "By_conv = create_convnet(do_bayes = True)\n",
        "By_pred = tf.nn.softmax_cross_entropy_with_logits(labels=y_true, logits=By_conv)\n",
        "Bcross_entropy = tf.reduce_mean(By_pred)\n",
        "\n",
        "# Optimizer\n",
        "Btrain_step = tf.train.AdamOptimizer(1e-4).minimize(Bcross_entropy)\n",
        "\n",
        "# Evaluation\n",
        "Bcorrect_prediction = tf.equal(tf.argmax(By_conv, 1), tf.argmax(y_true, 1))\n",
        "Baccuracy = tf.reduce_mean(tf.cast(Bcorrect_prediction, tf.float32))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "12mU6-sPEpT1",
        "colab_type": "code",
        "outputId": "2d86aa0d-c643-4300-e6f6-554d872a0fc1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        }
      },
      "cell_type": "code",
      "source": [
        "## Train - MNIST\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "for i in range(int(Ntrain_steps)+1):\n",
        "    batch = mnist.train.next_batch(64)\n",
        "\n",
        "    # Training\n",
        "    _, loss = sess.run([Btrain_step, Bcross_entropy], \n",
        "                       feed_dict={x: batch[0], y_true: batch[1], \n",
        "                                  keep_prob: float(bayesian_keep_prob)})\n",
        "    \n",
        "    # Status report\n",
        "    if i % 1000 == 0:\n",
        "        train_acc = sess.run(Baccuracy, feed_dict={x: batch[0], \n",
        "                                                  y_true: batch[1], \n",
        "                                                  keep_prob: float(1.0)})\n",
        "        \n",
        "        test_acc = sess.run(Baccuracy, feed_dict={x: mnist.test.images,\n",
        "                                                  y_true: mnist.test.labels, \n",
        "                                                  keep_prob: 1.0})\n",
        "        info = [str(tx) for tx in [i, loss, train_acc, test_acc] ]\n",
        "        print('\\t'.join(info))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\t6.106655\t0.125\t0.1657\n",
            "1000\t0.2945533\t0.953125\t0.958\n",
            "2000\t0.278653\t1.0\t0.9735\n",
            "3000\t0.3566081\t1.0\t0.9804\n",
            "4000\t0.29591033\t1.0\t0.9835\n",
            "5000\t0.25739187\t0.984375\t0.9854\n",
            "6000\t0.2624533\t1.0\t0.9883\n",
            "7000\t0.3505632\t1.0\t0.9882\n",
            "8000\t0.2769012\t0.984375\t0.9902\n",
            "9000\t0.120537475\t1.0\t0.991\n",
            "10000\t0.17430952\t1.0\t0.9913\n",
            "11000\t0.18917799\t1.0\t0.9912\n",
            "12000\t0.24237102\t1.0\t0.9911\n",
            "13000\t0.2735782\t1.0\t0.9927\n",
            "14000\t0.20978107\t1.0\t0.9925\n",
            "15000\t0.20811015\t1.0\t0.9926\n",
            "16000\t0.26158676\t1.0\t0.9914\n",
            "17000\t0.25177842\t1.0\t0.9932\n",
            "18000\t0.13369304\t1.0\t0.9916\n",
            "19000\t0.16718253\t1.0\t0.9917\n",
            "20000\t0.13704832\t1.0\t0.9921\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OCb8xs3ou_nV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Uncertainties from stochastic forward passes**"
      ]
    },
    {
      "metadata": {
        "id": "4InJYDy_u6ao",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Stochastic forward passes\n",
        "samples = np.zeros( shape=(N, mnist.test.images.shape[0], 10))\n",
        "\n",
        "for n in range(N):\n",
        "    logits = sess.run(By_conv, feed_dict={x: mnist.test.images, \n",
        "                                          y_true: mnist.test.labels,\n",
        "                                         keep_prob: bayesian_keep_prob})\n",
        "    samples[n] = softmax(logits)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o7MRUgoX0dBH",
        "colab_type": "code",
        "outputId": "fef888c3-0a06-4790-8c09-1e7cb75058a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "cell_type": "code",
      "source": [
        "# Variation ratio\n",
        "vr = var_ratio(samples)\n",
        "\n",
        "# Predictive entropy\n",
        "pe = pred_ent(samples)\n",
        "\n",
        "# Mutual information\n",
        "mi = mut_inf(samples)\n",
        "\n",
        "mi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-41.50894, -37.85634, -41.85383, ..., -29.22932, -38.40325,\n",
              "       -30.23868])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "MC-X4bfoGlYj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Bayesian predictions\n",
        "samples_avg = np.sum(samples, axis=0) / N\n",
        "labels_bayes = np.argmax(samples_avg, axis=1)\n",
        "probs_bayes = np.max(samples_avg, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dQwajwbIGo6R",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Comparison: Bayes vs Trad"
      ]
    },
    {
      "metadata": {
        "id": "8tQjFLGlGoE4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ref = np.argmax(mnist.test.labels, axis=1)\n",
        "data = { 'true': ref.tolist(),\n",
        "         'labels_trad': labels_trad.tolist(),\n",
        "         'probs_trad': probs_trad.tolist(),\n",
        "         'labels_bayes': labels_bayes.tolist(),\n",
        "         'probs_bayes': probs_bayes.tolist(),\n",
        "         'vr': vr.tolist(),\n",
        "         'pe': pe.tolist(),\n",
        "         'mi': mi.tolist()}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "df = df.assign(correct_trad = df.true == df.labels_trad)\n",
        "df = df.assign(correct_bayes = df.true == df.labels_bayes)\n",
        "\n",
        "df_both_wrong = df[(df.correct_trad == False) & (df.correct_bayes == False)]\n",
        "df_both_right = df[(df.correct_trad == True) & (df.correct_bayes == True)]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w_sxI7O8JFEJ",
        "colab_type": "code",
        "outputId": "20a81ace-f0c5-4083-baa0-cd7e3ff9836f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "cell_type": "code",
      "source": [
        "def abline(slope, intercept):\n",
        "    \"\"\"Plot a line from slope and intercept\"\"\"\n",
        "    axes = plt.gca()\n",
        "    x_vals = np.array(axes.get_xlim())\n",
        "    y_vals = intercept + slope * x_vals\n",
        "    plt.plot(x_vals, y_vals, '--', color = \"red\")\n",
        "\n",
        "# Best guess\n",
        "# Traditional approach is much more confidennt than Bayesian approach\n",
        "ax = df.plot.scatter(x = 'probs_trad', y = 'probs_bayes')\n",
        "ax.set_xlim(0.2, 1)\n",
        "ax.set_ylim(0.2, 1)\n",
        "ax.set_xlabel(\"Softmax point estimate for probability\")\n",
        "ax.set_ylabel(\"Bayesian point estimate for probability\")\n",
        "abline(1, 0)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEKCAYAAAD5MJl4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fbb1c450668>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "Aa4HHzfYJpEn",
        "colab_type": "code",
        "outputId": "fb249bbd-9367-4aa9-acfd-ef85c46c3c0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "cell_type": "code",
      "source": [
        "# Things both got wrong\n",
        "ax = df_both_wrong.plot.scatter(x = 'probs_trad', y = 'probs_bayes')\n",
        "ax.set_xlim(0.2, 1)\n",
        "ax.set_ylim(0.2, 1)\n",
        "ax.set_xlabel(\"Softmax point estimate for probability\")\n",
        "ax.set_ylabel(\"Bayesian point estimate for probability\")\n",
        "abline(1, 0)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEKCAYAAAD5MJl4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/h8RZQq+Cv3ZegAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fbb78981d30>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "ucWIbftHTv4L",
        "colab_type": "code",
        "outputId": "579d3d98-c28d-4cd1-f2c2-eaed74fc2d24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "cell_type": "code",
      "source": [
        "# Uncertainty measure for correct / incorrectly predicted digits\n",
        "df_bayes_correct = df[df.correct_bayes == True]\n",
        "df_bayes_wrong = df[df.correct_bayes == False]\n",
        "\n",
        "cum_c = np.cumsum(df_bayes_correct.pe)\n",
        "cum_w = np.cumsum(df_bayes_wrong.pe)\n",
        "\n",
        "\n",
        "print(sum(df_bayes_correct.pe) / len(df_bayes_correct))\n",
        "print(sum(df_bayes_wrong.pe) / len(df_bayes_wrong))\n",
        "\n",
        "df.to_csv('/content/drive/My Drive/Colab Notebooks/out.csv')\n",
        "\n",
        "\n",
        "\n",
        "# awk -F, '{ if($11==\"True\") {print $5}}' out.csv  | sort -k1,1gr > pe.true\n",
        "# awk -F, '{ if($11==\"False\") {print $5}}' out.csv  | sort -k1,1gr > pe.false\n",
        "# plot 'pe.true' u 1:($0/9871) title \"Correct\", 'pe.false' u 1:($0/129) title \"Incorrect\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6090864855167594\n",
            "1.2950307956162894\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "n83fotZOJc-U",
        "colab_type": "code",
        "outputId": "42803621-d002-4ba4-c9f5-5d31a44b6dc0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1940
        }
      },
      "cell_type": "code",
      "source": [
        "k = 9729\n",
        "# 9729\t6\t6\t-36.121928\t0.768686\t0.829598\t0.995529\t5\t0.82\tFalse\tFalse\n",
        "plt.imshow(mnist.test.images[k].reshape(28,28))\n",
        "plt.show()\n",
        "print(mnist.test.labels[k])\n",
        "df_both_wrong"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPoAAAD4CAYAAADFJPs2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fbb188889b0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>labels_bayes</th>\n",
              "      <th>labels_trad</th>\n",
              "      <th>mi</th>\n",
              "      <th>pe</th>\n",
              "      <th>probs_bayes</th>\n",
              "      <th>probs_trad</th>\n",
              "      <th>true</th>\n",
              "      <th>vr</th>\n",
              "      <th>correct_trad</th>\n",
              "      <th>correct_bayes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>259</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-167.394347</td>\n",
              "      <td>1.515802</td>\n",
              "      <td>0.515170</td>\n",
              "      <td>0.764494</td>\n",
              "      <td>6</td>\n",
              "      <td>0.655</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>445</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-123.320945</td>\n",
              "      <td>1.173245</td>\n",
              "      <td>0.693784</td>\n",
              "      <td>0.996533</td>\n",
              "      <td>6</td>\n",
              "      <td>0.860</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>582</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>-58.441600</td>\n",
              "      <td>0.665186</td>\n",
              "      <td>0.863769</td>\n",
              "      <td>0.936639</td>\n",
              "      <td>8</td>\n",
              "      <td>0.915</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>583</th>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>-98.772690</td>\n",
              "      <td>0.976967</td>\n",
              "      <td>0.592227</td>\n",
              "      <td>0.577771</td>\n",
              "      <td>2</td>\n",
              "      <td>0.640</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>659</th>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>-203.434999</td>\n",
              "      <td>1.553480</td>\n",
              "      <td>0.467368</td>\n",
              "      <td>0.806227</td>\n",
              "      <td>2</td>\n",
              "      <td>0.595</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>947</th>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>-96.769333</td>\n",
              "      <td>0.915554</td>\n",
              "      <td>0.805700</td>\n",
              "      <td>0.943317</td>\n",
              "      <td>8</td>\n",
              "      <td>0.830</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1014</th>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>-114.718100</td>\n",
              "      <td>1.067069</td>\n",
              "      <td>0.740422</td>\n",
              "      <td>0.978160</td>\n",
              "      <td>6</td>\n",
              "      <td>0.785</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1226</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>-45.967080</td>\n",
              "      <td>0.647635</td>\n",
              "      <td>0.874662</td>\n",
              "      <td>0.937546</td>\n",
              "      <td>7</td>\n",
              "      <td>0.950</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1232</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>-142.337072</td>\n",
              "      <td>1.184733</td>\n",
              "      <td>0.719963</td>\n",
              "      <td>0.752795</td>\n",
              "      <td>9</td>\n",
              "      <td>0.845</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1247</th>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>-215.882795</td>\n",
              "      <td>1.971716</td>\n",
              "      <td>0.395604</td>\n",
              "      <td>0.869668</td>\n",
              "      <td>9</td>\n",
              "      <td>0.495</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1260</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-107.493610</td>\n",
              "      <td>0.966492</td>\n",
              "      <td>0.618793</td>\n",
              "      <td>0.695135</td>\n",
              "      <td>7</td>\n",
              "      <td>0.695</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1319</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-197.954422</td>\n",
              "      <td>1.665378</td>\n",
              "      <td>0.442753</td>\n",
              "      <td>0.762782</td>\n",
              "      <td>8</td>\n",
              "      <td>0.595</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1393</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>-201.690479</td>\n",
              "      <td>1.789796</td>\n",
              "      <td>0.342150</td>\n",
              "      <td>0.955029</td>\n",
              "      <td>5</td>\n",
              "      <td>0.440</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1522</th>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>-224.208208</td>\n",
              "      <td>1.796159</td>\n",
              "      <td>0.339965</td>\n",
              "      <td>0.783155</td>\n",
              "      <td>7</td>\n",
              "      <td>0.395</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1709</th>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>-196.665659</td>\n",
              "      <td>1.557790</td>\n",
              "      <td>0.573248</td>\n",
              "      <td>0.558800</td>\n",
              "      <td>9</td>\n",
              "      <td>0.750</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1878</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>-112.070125</td>\n",
              "      <td>1.066095</td>\n",
              "      <td>0.585646</td>\n",
              "      <td>0.981964</td>\n",
              "      <td>8</td>\n",
              "      <td>0.640</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1901</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>-57.714791</td>\n",
              "      <td>0.681298</td>\n",
              "      <td>0.864344</td>\n",
              "      <td>0.998459</td>\n",
              "      <td>9</td>\n",
              "      <td>0.900</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2035</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>-87.783225</td>\n",
              "      <td>0.895622</td>\n",
              "      <td>0.719616</td>\n",
              "      <td>0.999710</td>\n",
              "      <td>5</td>\n",
              "      <td>0.760</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2098</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-74.676480</td>\n",
              "      <td>0.830496</td>\n",
              "      <td>0.527012</td>\n",
              "      <td>0.747699</td>\n",
              "      <td>2</td>\n",
              "      <td>0.555</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2118</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-153.119428</td>\n",
              "      <td>1.451359</td>\n",
              "      <td>0.635872</td>\n",
              "      <td>0.994892</td>\n",
              "      <td>6</td>\n",
              "      <td>0.850</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2130</th>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>-125.205160</td>\n",
              "      <td>1.164540</td>\n",
              "      <td>0.680328</td>\n",
              "      <td>0.998195</td>\n",
              "      <td>4</td>\n",
              "      <td>0.735</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2135</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-146.055239</td>\n",
              "      <td>1.229158</td>\n",
              "      <td>0.674646</td>\n",
              "      <td>0.939963</td>\n",
              "      <td>6</td>\n",
              "      <td>0.850</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2293</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>-179.137691</td>\n",
              "      <td>2.002582</td>\n",
              "      <td>0.385912</td>\n",
              "      <td>0.737216</td>\n",
              "      <td>9</td>\n",
              "      <td>0.425</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2369</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>-138.789261</td>\n",
              "      <td>1.239987</td>\n",
              "      <td>0.612425</td>\n",
              "      <td>0.961810</td>\n",
              "      <td>5</td>\n",
              "      <td>0.685</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2387</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-118.110454</td>\n",
              "      <td>1.023786</td>\n",
              "      <td>0.619133</td>\n",
              "      <td>0.629290</td>\n",
              "      <td>9</td>\n",
              "      <td>0.690</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2414</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>-138.219717</td>\n",
              "      <td>1.210537</td>\n",
              "      <td>0.679789</td>\n",
              "      <td>0.756964</td>\n",
              "      <td>9</td>\n",
              "      <td>0.755</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2462</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-133.572135</td>\n",
              "      <td>1.228591</td>\n",
              "      <td>0.609093</td>\n",
              "      <td>0.690861</td>\n",
              "      <td>2</td>\n",
              "      <td>0.750</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2488</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>-172.622828</td>\n",
              "      <td>1.488403</td>\n",
              "      <td>0.604682</td>\n",
              "      <td>0.939769</td>\n",
              "      <td>2</td>\n",
              "      <td>0.740</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2597</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>-134.669829</td>\n",
              "      <td>1.301245</td>\n",
              "      <td>0.671129</td>\n",
              "      <td>0.999972</td>\n",
              "      <td>5</td>\n",
              "      <td>0.775</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2654</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-47.541289</td>\n",
              "      <td>0.637968</td>\n",
              "      <td>0.877041</td>\n",
              "      <td>0.999989</td>\n",
              "      <td>6</td>\n",
              "      <td>0.960</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2896</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-78.127539</td>\n",
              "      <td>0.838861</td>\n",
              "      <td>0.825967</td>\n",
              "      <td>0.990579</td>\n",
              "      <td>8</td>\n",
              "      <td>0.990</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2939</th>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>-206.649808</td>\n",
              "      <td>2.011286</td>\n",
              "      <td>0.365750</td>\n",
              "      <td>0.973193</td>\n",
              "      <td>9</td>\n",
              "      <td>0.395</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2953</th>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>-149.885345</td>\n",
              "      <td>1.422956</td>\n",
              "      <td>0.626880</td>\n",
              "      <td>0.969150</td>\n",
              "      <td>3</td>\n",
              "      <td>0.705</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3073</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>-70.798462</td>\n",
              "      <td>0.809702</td>\n",
              "      <td>0.732192</td>\n",
              "      <td>0.854674</td>\n",
              "      <td>1</td>\n",
              "      <td>0.745</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3422</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-57.479129</td>\n",
              "      <td>0.729730</td>\n",
              "      <td>0.847058</td>\n",
              "      <td>0.993935</td>\n",
              "      <td>6</td>\n",
              "      <td>0.960</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3520</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>-95.846696</td>\n",
              "      <td>1.045607</td>\n",
              "      <td>0.768003</td>\n",
              "      <td>0.943097</td>\n",
              "      <td>6</td>\n",
              "      <td>0.845</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3558</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-102.493003</td>\n",
              "      <td>0.966213</td>\n",
              "      <td>0.777168</td>\n",
              "      <td>0.992474</td>\n",
              "      <td>5</td>\n",
              "      <td>0.965</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3808</th>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>-195.481214</td>\n",
              "      <td>1.778999</td>\n",
              "      <td>0.370384</td>\n",
              "      <td>0.988927</td>\n",
              "      <td>7</td>\n",
              "      <td>0.385</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4497</th>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>-62.508113</td>\n",
              "      <td>0.728848</td>\n",
              "      <td>0.811485</td>\n",
              "      <td>0.706661</td>\n",
              "      <td>8</td>\n",
              "      <td>0.805</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4740</th>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>-179.062595</td>\n",
              "      <td>1.517483</td>\n",
              "      <td>0.580809</td>\n",
              "      <td>0.765403</td>\n",
              "      <td>3</td>\n",
              "      <td>0.700</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4761</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>-202.496101</td>\n",
              "      <td>1.732625</td>\n",
              "      <td>0.512019</td>\n",
              "      <td>0.757255</td>\n",
              "      <td>9</td>\n",
              "      <td>0.650</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4807</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-214.256891</td>\n",
              "      <td>2.054994</td>\n",
              "      <td>0.328946</td>\n",
              "      <td>0.415849</td>\n",
              "      <td>8</td>\n",
              "      <td>0.475</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4823</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>-101.443558</td>\n",
              "      <td>0.965583</td>\n",
              "      <td>0.788720</td>\n",
              "      <td>0.514418</td>\n",
              "      <td>9</td>\n",
              "      <td>0.855</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4956</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>-80.075772</td>\n",
              "      <td>0.868138</td>\n",
              "      <td>0.488187</td>\n",
              "      <td>0.672068</td>\n",
              "      <td>8</td>\n",
              "      <td>0.515</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5937</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>-98.999407</td>\n",
              "      <td>0.945677</td>\n",
              "      <td>0.747426</td>\n",
              "      <td>0.981240</td>\n",
              "      <td>5</td>\n",
              "      <td>0.805</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6576</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-171.125564</td>\n",
              "      <td>1.373375</td>\n",
              "      <td>0.625172</td>\n",
              "      <td>0.965494</td>\n",
              "      <td>7</td>\n",
              "      <td>0.845</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6597</th>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>-130.700233</td>\n",
              "      <td>1.210922</td>\n",
              "      <td>0.718619</td>\n",
              "      <td>0.849234</td>\n",
              "      <td>0</td>\n",
              "      <td>0.810</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6847</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>-98.990767</td>\n",
              "      <td>0.985276</td>\n",
              "      <td>0.504342</td>\n",
              "      <td>0.732074</td>\n",
              "      <td>6</td>\n",
              "      <td>0.545</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8408</th>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>-125.336662</td>\n",
              "      <td>1.130755</td>\n",
              "      <td>0.742124</td>\n",
              "      <td>0.904430</td>\n",
              "      <td>8</td>\n",
              "      <td>0.815</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9009</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>-51.583443</td>\n",
              "      <td>0.681464</td>\n",
              "      <td>0.843597</td>\n",
              "      <td>0.926131</td>\n",
              "      <td>7</td>\n",
              "      <td>0.860</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9587</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>-195.795535</td>\n",
              "      <td>1.854903</td>\n",
              "      <td>0.406274</td>\n",
              "      <td>0.712931</td>\n",
              "      <td>9</td>\n",
              "      <td>0.510</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9634</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-189.206796</td>\n",
              "      <td>2.014856</td>\n",
              "      <td>0.376636</td>\n",
              "      <td>0.817042</td>\n",
              "      <td>0</td>\n",
              "      <td>0.495</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9642</th>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>-150.508077</td>\n",
              "      <td>1.271424</td>\n",
              "      <td>0.612753</td>\n",
              "      <td>0.692598</td>\n",
              "      <td>9</td>\n",
              "      <td>0.725</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9679</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>-180.101303</td>\n",
              "      <td>2.269276</td>\n",
              "      <td>0.174606</td>\n",
              "      <td>0.569967</td>\n",
              "      <td>6</td>\n",
              "      <td>0.255</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9729</th>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>-36.185948</td>\n",
              "      <td>0.514310</td>\n",
              "      <td>0.904201</td>\n",
              "      <td>0.999057</td>\n",
              "      <td>5</td>\n",
              "      <td>0.880</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      labels_bayes  labels_trad          mi        pe  probs_bayes  \\\n",
              "259              0            0 -167.394347  1.515802     0.515170   \n",
              "445              0            0 -123.320945  1.173245     0.693784   \n",
              "582              2            2  -58.441600  0.665186     0.863769   \n",
              "583              7            7  -98.772690  0.976967     0.592227   \n",
              "659              7            1 -203.434999  1.553480     0.467368   \n",
              "947              9            9  -96.769333  0.915554     0.805700   \n",
              "1014             5            5 -114.718100  1.067069     0.740422   \n",
              "1226             2            2  -45.967080  0.647635     0.874662   \n",
              "1232             4            4 -142.337072  1.184733     0.719963   \n",
              "1247             5            5 -215.882795  1.971716     0.395604   \n",
              "1260             1            1 -107.493610  0.966492     0.618793   \n",
              "1319             0            0 -197.954422  1.665378     0.442753   \n",
              "1393             3            3 -201.690479  1.789796     0.342150   \n",
              "1522             9            9 -224.208208  1.796159     0.339965   \n",
              "1709             5            3 -196.665659  1.557790     0.573248   \n",
              "1878             3            3 -112.070125  1.066095     0.585646   \n",
              "1901             4            4  -57.714791  0.681298     0.864344   \n",
              "2035             3            3  -87.783225  0.895622     0.719616   \n",
              "2098             0            0  -74.676480  0.830496     0.527012   \n",
              "2118             0            0 -153.119428  1.451359     0.635872   \n",
              "2130             9            9 -125.205160  1.164540     0.680328   \n",
              "2135             1            1 -146.055239  1.229158     0.674646   \n",
              "2293             4            4 -179.137691  2.002582     0.385912   \n",
              "2369             3            3 -138.789261  1.239987     0.612425   \n",
              "2387             1            1 -118.110454  1.023786     0.619133   \n",
              "2414             4            4 -138.219717  1.210537     0.679789   \n",
              "2462             0            0 -133.572135  1.228591     0.609093   \n",
              "2488             4            4 -172.622828  1.488403     0.604682   \n",
              "2597             3            3 -134.669829  1.301245     0.671129   \n",
              "2654             1            1  -47.541289  0.637968     0.877041   \n",
              "2896             0            0  -78.127539  0.838861     0.825967   \n",
              "2939             5            5 -206.649808  2.011286     0.365750   \n",
              "2953             5            5 -149.885345  1.422956     0.626880   \n",
              "3073             2            2  -70.798462  0.809702     0.732192   \n",
              "3422             0            0  -57.479129  0.729730     0.847058   \n",
              "3520             4            4  -95.846696  1.045607     0.768003   \n",
              "3558             0            0 -102.493003  0.966213     0.777168   \n",
              "3808             8            8 -195.481214  1.778999     0.370384   \n",
              "4497             7            7  -62.508113  0.728848     0.811485   \n",
              "4740             5            5 -179.062595  1.517483     0.580809   \n",
              "4761             4            4 -202.496101  1.732625     0.512019   \n",
              "4807             0            0 -214.256891  2.054994     0.328946   \n",
              "4823             4            4 -101.443558  0.965583     0.788720   \n",
              "4956             4            4  -80.075772  0.868138     0.488187   \n",
              "5937             3            3  -98.999407  0.945677     0.747426   \n",
              "6576             1            1 -171.125564  1.373375     0.625172   \n",
              "6597             7            7 -130.700233  1.210922     0.718619   \n",
              "6847             4            4  -98.990767  0.985276     0.504342   \n",
              "8408             5            5 -125.336662  1.130755     0.742124   \n",
              "9009             2            2  -51.583443  0.681464     0.843597   \n",
              "9587             4            4 -195.795535  1.854903     0.406274   \n",
              "9634             1            1 -189.206796  2.014856     0.376636   \n",
              "9642             7            7 -150.508077  1.271424     0.612753   \n",
              "9679             1            3 -180.101303  2.269276     0.174606   \n",
              "9729             6            6  -36.185948  0.514310     0.904201   \n",
              "\n",
              "      probs_trad  true     vr  correct_trad  correct_bayes  \n",
              "259     0.764494     6  0.655         False          False  \n",
              "445     0.996533     6  0.860         False          False  \n",
              "582     0.936639     8  0.915         False          False  \n",
              "583     0.577771     2  0.640         False          False  \n",
              "659     0.806227     2  0.595         False          False  \n",
              "947     0.943317     8  0.830         False          False  \n",
              "1014    0.978160     6  0.785         False          False  \n",
              "1226    0.937546     7  0.950         False          False  \n",
              "1232    0.752795     9  0.845         False          False  \n",
              "1247    0.869668     9  0.495         False          False  \n",
              "1260    0.695135     7  0.695         False          False  \n",
              "1319    0.762782     8  0.595         False          False  \n",
              "1393    0.955029     5  0.440         False          False  \n",
              "1522    0.783155     7  0.395         False          False  \n",
              "1709    0.558800     9  0.750         False          False  \n",
              "1878    0.981964     8  0.640         False          False  \n",
              "1901    0.998459     9  0.900         False          False  \n",
              "2035    0.999710     5  0.760         False          False  \n",
              "2098    0.747699     2  0.555         False          False  \n",
              "2118    0.994892     6  0.850         False          False  \n",
              "2130    0.998195     4  0.735         False          False  \n",
              "2135    0.939963     6  0.850         False          False  \n",
              "2293    0.737216     9  0.425         False          False  \n",
              "2369    0.961810     5  0.685         False          False  \n",
              "2387    0.629290     9  0.690         False          False  \n",
              "2414    0.756964     9  0.755         False          False  \n",
              "2462    0.690861     2  0.750         False          False  \n",
              "2488    0.939769     2  0.740         False          False  \n",
              "2597    0.999972     5  0.775         False          False  \n",
              "2654    0.999989     6  0.960         False          False  \n",
              "2896    0.990579     8  0.990         False          False  \n",
              "2939    0.973193     9  0.395         False          False  \n",
              "2953    0.969150     3  0.705         False          False  \n",
              "3073    0.854674     1  0.745         False          False  \n",
              "3422    0.993935     6  0.960         False          False  \n",
              "3520    0.943097     6  0.845         False          False  \n",
              "3558    0.992474     5  0.965         False          False  \n",
              "3808    0.988927     7  0.385         False          False  \n",
              "4497    0.706661     8  0.805         False          False  \n",
              "4740    0.765403     3  0.700         False          False  \n",
              "4761    0.757255     9  0.650         False          False  \n",
              "4807    0.415849     8  0.475         False          False  \n",
              "4823    0.514418     9  0.855         False          False  \n",
              "4956    0.672068     8  0.515         False          False  \n",
              "5937    0.981240     5  0.805         False          False  \n",
              "6576    0.965494     7  0.845         False          False  \n",
              "6597    0.849234     0  0.810         False          False  \n",
              "6847    0.732074     6  0.545         False          False  \n",
              "8408    0.904430     8  0.815         False          False  \n",
              "9009    0.926131     7  0.860         False          False  \n",
              "9587    0.712931     9  0.510         False          False  \n",
              "9634    0.817042     0  0.495         False          False  \n",
              "9642    0.692598     9  0.725         False          False  \n",
              "9679    0.569967     6  0.255         False          False  \n",
              "9729    0.999057     5  0.880         False          False  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "metadata": {
        "id": "qXNQDXzXPxDk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## EMnist\n",
        "We know that in this case, the data set is out of distribution. Usually, we don't have this information."
      ]
    },
    {
      "metadata": {
        "id": "4QvEpb9nGdRZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Download data set from here: https://www.nist.gov/itl/iad/image-group/emnist-dataset\n",
        "# Upload to gDrive\n",
        "# Copy from gDrive\n",
        "!cp '/content/drive/My Drive/Colab Notebooks/emnist.zip' .\n",
        "\n",
        "!rm -fr gzip\n",
        "\n",
        "# Unzip\n",
        "!unzip emnist.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XTa37vlGB0sT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import gzip\n",
        "\n",
        "def _read32(bytestream):\n",
        "    dt = np.dtype(np.uint32).newbyteorder('>')\n",
        "    return np.frombuffer(bytestream.read(4), dtype=dt)[0]\n",
        "\n",
        "\n",
        "def extract_images(filename):\n",
        "    with gzip.open(filename) as bytestream:\n",
        "        magic = _read32(bytestream)\n",
        "        if magic != 2051:\n",
        "            raise ValueError(\n",
        "                'Invalid magic number %d in MNIST image file: %s' %\n",
        "                (magic, filename))\n",
        "        num_images = _read32(bytestream)\n",
        "        rows = _read32(bytestream)\n",
        "        cols = _read32(bytestream)\n",
        "        buf = bytestream.read(rows * cols * num_images)\n",
        "        data = np.frombuffer(buf, dtype=np.uint8)\n",
        "        data = data.reshape(num_images, rows, cols, 1)\n",
        "        return data\n",
        "\n",
        "\n",
        "def extract_labels(filename):\n",
        "    with gzip.open(filename) as bytestream:\n",
        "        magic = _read32(bytestream)\n",
        "        if magic != 2049:\n",
        "            raise ValueError(\n",
        "              'Invalid magic number %d in MNIST label file: %s' %\n",
        "              (magic, filename))\n",
        "        num_items = _read32(bytestream)\n",
        "        buf = bytestream.read(num_items)\n",
        "        labels = np.frombuffer(buf, dtype=np.uint8)\n",
        "        return labels\n",
        "\n",
        "\n",
        "def read_emnist(emnist_dir):\n",
        "\n",
        "    TRAIN_IMAGES = emnist_dir+'/emnist-balanced-train-images-idx3-ubyte.gz'\n",
        "    TRAIN_LABELS = emnist_dir+'/emnist-balanced-train-labels-idx1-ubyte.gz'\n",
        "    TEST_IMAGES = emnist_dir+'/emnist-balanced-test-images-idx3-ubyte.gz'\n",
        "    TEST_LABELS = emnist_dir+'/emnist-balanced-test-labels-idx1-ubyte.gz'\n",
        "    MAPPING = emnist_dir+'/emnist-balanced-mapping.txt'\n",
        "\n",
        "    train_images = extract_images(TRAIN_IMAGES)\n",
        "    train_labels = extract_labels(TRAIN_LABELS)\n",
        "    test_images = extract_images(TEST_IMAGES)\n",
        "    test_labels = extract_labels(TEST_LABELS)\n",
        "\n",
        "    with open(MAPPING, \"r\") as f:\n",
        "        mapping = f.readlines()\n",
        "        mapping = {str(x.split()[0]): str(x.split()[1]) for x in mapping}\n",
        "\n",
        "    # Convert to float32\n",
        "    train_images = train_images.astype('float32')\n",
        "    test_images = test_images.astype('float32')\n",
        "\n",
        "    # Normalize\n",
        "    train_images /= 255\n",
        "    test_images /= 255\n",
        "\n",
        "    # Output format: (28, 28, 1)\n",
        "    return ((train_images, train_labels), (test_images, test_labels), mapping)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lFrA4jG68tK6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "emnist = read_emnist('gzip')\n",
        "\n",
        "X = np.float32(emnist[1][0].reshape(18800, 784))\n",
        "y = np.float32(emnist[1][1])\n",
        "X.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lY48XWYKSBHp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Predict digits\n",
        "eN = 18800\n",
        "\n",
        "y_prob = tf.nn.softmax(y_conv)\n",
        "pred_trad = sess.run(y_prob, feed_dict={x: X,\n",
        "                                          y_true: mnist.test.labels, \n",
        "                                          keep_prob: float(1.0)})\n",
        "\n",
        "\n",
        "labels_trad = np.argmax(pred_trad, axis=1)\n",
        "probs_trad = np.max(pred_trad, axis=1)\n",
        "\n",
        "Eprobs_trad = probs_trad"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vxjTxUrtzGDH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Stochastic forward passes\n",
        "Esamples = np.zeros( shape=(N, X.shape[0], 10))\n",
        "\n",
        "for n in range(N):\n",
        "    logits = sess.run(By_conv, feed_dict={x: X, \n",
        "                                          y_true: mnist.test.labels[0:eN],\n",
        "                                          keep_prob: 0.9})\n",
        "    Esamples[n] = softmax(logits)\n",
        "\n",
        "Esamples_avg = np.sum(Esamples, axis=0) / N\n",
        "Elabels_bayes = np.argmax(Esamples_avg, axis=1)\n",
        "Eprobs_bayes = np.max(Esamples_avg, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_2o9p0gfUC4Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(Eprobs_trad.shape)\n",
        "print(Eprobs_bayes.shape)\n",
        "ax = plt.scatter(Eprobs_trad, Eprobs_bayes)\n",
        "plt.xlim(0.2, 1)\n",
        "plt.ylim(0.2, 1)\n",
        "plt.xlabel(\"Softmax point estimate for probability\")\n",
        "plt.ylabel(\"Bayesian point estimate for probability\")\n",
        "abline(1, 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "04-IeYa3VhAI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Pick some examples\n",
        "As = np.argwhere(y==15)\n",
        "\n",
        "i = 11215\n",
        "\n",
        "print(y[i])\n",
        "print(Eprobs_trad[i], \"; \")\n",
        "print(Eprobs_bayes[i], \"; \", Elabels_bayes[i])\n",
        "\n",
        "plt.imshow(X[i].reshape((28,28)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M5yEoJGCvQn4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "As"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Wyy2C8oI3nZm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}